{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659326cb",
   "metadata": {},
   "source": [
    "# Collecting and processing data from the Facebook Marketing API\n",
    "\n",
    "*Authors*: Yuanmo He & Milena Tsvetkova\n",
    "\n",
    "In this demo, we will collect location, age, and gender demographics for Facebook users who have a specific brand as interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f448ff1",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28899f31",
   "metadata": {},
   "source": [
    "## 1. Install `pysocialwatcher`\n",
    "\n",
    "1. Open the Terminal in VS Code and install `git` in the virtual environment.\n",
    "\n",
    "`conda install git`\n",
    "\n",
    "2. Clone the project.\n",
    "\n",
    "`git clone https://github.com/joaopalotti/pySocialWatcher.git`\n",
    "\n",
    "3. Go to the project directory.\n",
    "\n",
    "`cd [path to package directory that contains setup.py]`\n",
    "\n",
    "4. Install the package.\n",
    "\n",
    "`python setup.py install`\n",
    "\n",
    "5. If you get errors that certain packages are missing, you should run in the Terminal.\n",
    "\n",
    "  `conda install tabulate`\n",
    "  \n",
    "  `conda install coloredlogs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b02ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the library pysocialwatcher to query Facebook Marketing watcherAPI\n",
    "# Details about the library: https://github.com/joaopalotti/pySocialWatcher\n",
    "# Quick intro: https://goo.gl/WzE9ic\n",
    "# Additional guidance: https://worldbank.github.io/connectivity_mapping/intro.html\n",
    "\n",
    "from pysocialwatcher import watcherAPI\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b998377",
   "metadata": {},
   "source": [
    "## 2. Get your Facebook developer credentials\n",
    "\n",
    "1. Create a **\"Meta for Developers\" account**.\n",
    "     1. Go to https://developers.facebook.com/ and register with your Facebook account and phone number.\n",
    "     2. Click on `New app` -> `Other` -> `Business account` -> `Marketing API` on the page *Add products to your app*\n",
    "2. Get your **access token**.\n",
    "     1. In the left panel select `Tools` -> `Get Access Token` -> Select the options starting with `ads_` -> Click on `Get Token`\n",
    "     2. Copy the token in a csv file. The token is a long alphanumeric string.\n",
    "3. Get your **application ID**.\n",
    "     1. Go to https://www.facebook.com/business/ and click on `Create Advert`\n",
    "     2. In the URL, copy the number after `act=` such as: https://www.facebook.com/ads/manager/creation/creation/?act=**952863473440**&pid=p1\n",
    "     3. Copy that number in the csv file as a second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69073086",
   "metadata": {},
   "outputs": [],
   "source": [
    "watcher = watcherAPI(api_version=\"15.0\")\n",
    "\n",
    "# Access your Facebook credentials from the file fb_credentials.csv \n",
    "# Access Token goes in the first column, Application ID is in the second column \n",
    "watcher.load_credentials_file(\"data/fb_credentials.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85483b52",
   "metadata": {},
   "source": [
    "## 3. Get the brands' interest ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949190ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Collect the Facebook interest ids for the brands\n",
    "brands_on_twitter = pd.read_csv(\"data/brands_on_twitter.csv\")\n",
    "brands = brands_on_twitter['brand'][:5]  # Just the first 5 for the demo\n",
    "rowlst = []\n",
    "errorlst = []\n",
    "\n",
    "# We'll catch any errors due to names not being found\n",
    "for i in range(len(brands)):   \n",
    "    try:\n",
    "        rowlst.append(dict(watcher.get_interests_given_query(brands[i]).loc[0]))\n",
    "    except Exception as e:\n",
    "        errorlst.append({i: e})\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dab83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 5\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "print('Found:', len(rowlst)) \n",
    "print('Errors:', len(errorlst)) \n",
    "\n",
    "# No errors here but otherwise, you need to resolve them manually\n",
    "# Errors could be due to different spelling or no/deleted account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109a8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interest id and check errors manually \n",
    "fb_interest_id = pd.DataFrame(rowlst)\n",
    "error_ids = [list(i.keys())[0] for i in errorlst]\n",
    "fb_interest_id.insert(0, 'brand', brands.drop(error_ids).reset_index(drop = True))\n",
    "fb_interest_id = fb_interest_id.rename(columns = {'name': \"FB_name\"})\n",
    "fb_interest_id.to_excel('data/FB_interest_id_raw.xlsx', index = False) \n",
    "\n",
    "# Label the weird brands in FB_interest_id_raw.xlsx,\n",
    "# manually check and modify in FB_interest_id.xlsx.\n",
    "# Also manually add the ones that cannot be found automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d343281",
   "metadata": {},
   "source": [
    "## 4. Create `json` files for data collection\n",
    "We use code to create the list of interest ids. Age, gender, and location can be easily written manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_interest_id = pd.read_excel(\"data/FB_interest_id_raw.xlsx\")\n",
    "\n",
    "with open (\"data/interest_list.json\", \"a\") as file:\n",
    "    for index, row in fb_interest_id.iterrows():\n",
    "        json.dump({\"or\": [row['id']], \"name\": row['brand']}, file)\n",
    "        file.write(',\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d25325",
   "metadata": {},
   "source": [
    "We are not sharing the interest ids of the brands (the content of interest_list.json), as it is not clear from Facebook whether we have the rights to share them. But you should be able to get them with the code provided so far. Then, use the json files as intructed by pysocialwatcher https://github.com/joaopalotti/pySocialWatcher to collect the data. The following cell provides the example json contents to be saved in the correponding json files."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e508daa",
   "metadata": {},
   "source": [
    "# collecting_urban.json\n",
    "{\"name\": \"all US vs only urban\",\n",
    "  \"geo_locations\": [\n",
    "      { \"name\": \"countries\", \"values\": [\"US\"] },\n",
    "      { \"name\": \"custom_locations\", \"values\": [{\"custom_type\": \"multi_city\", \"country\": \"US\"}]}\n",
    "  ],\n",
    "  \"genders\": [0],\n",
    "  \"ages_ranges\": [\n",
    "      {\"min\":18}\n",
    "  ],\n",
    "  \"interests\": [# put the content of interest_list.json here\n",
    "]\n",
    "}\n",
    "\n",
    "# collecting_gender.json\n",
    "{\"name\": \"gender\",\n",
    "  \"geo_locations\": [\n",
    "      { \"name\": \"countries\", \"values\": [\"US\"] }\n",
    "  ],\n",
    "  \"genders\": [1, 2],\n",
    "  \"ages_ranges\": [\n",
    "      {\"min\":18}\n",
    "  ],\n",
    "\"interests\": [# put the content of interest_list.json here\n",
    "]\n",
    "}\n",
    "\n",
    "# collecting_age.json\n",
    "{\"name\": \"age\",\n",
    "  \"geo_locations\": [\n",
    "      { \"name\": \"countries\", \"values\": [\"US\"] }\n",
    "  ],\n",
    "  \"genders\": [0],\n",
    "  \"ages_ranges\": [\n",
    "      {\"min\":18, \"max\": 24},\n",
    "      {\"min\":25, \"max\": 34},\n",
    "      {\"min\":35, \"max\": 44},\n",
    "      {\"min\":45, \"max\": 54},\n",
    "      {\"min\":55, \"max\": 64},\n",
    "      {\"min\":65}\n",
    "  ],\n",
    " \"interests\": [# put the content of interest_list.json here\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7d5e6",
   "metadata": {},
   "source": [
    "## 5. Collect data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_collect_list = [\"data/collecting_urban.json\"] #, \"data/collecting_gender.json\", \"data/collecting_age.json\"]\n",
    "collected_files = [\"data/collected_urban.csv\"] #, \"data/collected_gender.csv\", \"data/collected_age.csv\"]\n",
    "\n",
    "for i in range(len(to_collect_list)):\n",
    "    \n",
    "    collecting = to_collect_list[i]\n",
    "    collected = collected_files[i]\n",
    "    \n",
    "    print(\"start\", collecting, \"---\")\n",
    "    watcher = watcherAPI(api_version=\"15.0\", outputname = collected)\n",
    "    watcher.load_credentials_file(\"data/FB_credentials.csv\")\n",
    "    \n",
    "    df = watcher.run_data_collection(collecting, remove_tmp_files = True)\n",
    "    \n",
    "    print(collected, \"end---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918da7c6",
   "metadata": {},
   "source": [
    "# Porcessing data\n",
    "\n",
    "Process the collected data and in the end get two files representing the upper and lower bounds for monthly active users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3630955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pivot(csvfile, key_column):\n",
    "    \"\"\"\n",
    "    Input the file name of the csv file and the column name of the variable (age, gender, etc.)\n",
    "    return the cleaned and pivoted version of dataframe for upper and lower mau\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csvfile, index_col = 0)\n",
    "    df = df[['interests', key_column, 'mau_audience_upper_bound', 'mau_audience_lower_bound']]\n",
    "    df = df.assign(id = df['interests'].str.extract(\"\\[([0-9]+)\\]\")) \n",
    "    \n",
    "    upper = df[['id', 'mau_audience_upper_bound', key_column]]\n",
    "    upper = upper.pivot(index = 'id', columns = key_column, values = 'mau_audience_upper_bound')\n",
    "    upper.columns.name = None\n",
    "    \n",
    "    lower = df[['id', 'mau_audience_lower_bound', key_column]]\n",
    "    lower = lower.pivot(index = 'id', columns = key_column, values = 'mau_audience_lower_bound')\n",
    "    lower.columns.name = None\n",
    "    \n",
    "    return upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process different categories individually\n",
    "upper_geo, lower_geo = clean_pivot(\"collected_urban.csv\", 'geo_locations')\n",
    "upper_geo.columns = ['all_US', 'urban']\n",
    "lower_geo.columns = ['all_US', 'urban']\n",
    "\n",
    "upper_gender, lower_gender = clean_pivot(\"collected_gender.csv\", 'genders')\n",
    "upper_gender.columns = ['male', 'female']\n",
    "lower_gender.columns = ['male', 'female']\n",
    "\n",
    "upper_age, lower_age = clean_pivot(\"collected_age.csv\", 'ages_ranges')\n",
    "upper_age.columns = ['age18_24', 'age25_34', 'age35_44', 'age45_54', 'age55_64', 'age65plus']\n",
    "lower_age.columns = ['age18_24', 'age25_34', 'age35_44', 'age45_54', 'age55_64', 'age65plus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ccbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together in two different dataframes\n",
    "upper_mau = pd.concat([upper_geo, upper_age, upper_gender], axis = 1)\n",
    "lower_mau = pd.concat([lower_geo, lower_age, lower_gender], axis = 1)\n",
    "\n",
    "FB_interet_id = pd.read_excel(\"data/FB_interest_id_raw.xlsx\", dtype = {'id': str})[['brand', 'id']]\n",
    "upper_mau = pd.merge(FB_interet_id, upper_mau, on = 'id').drop(columns = ['id'])\n",
    "lower_mau = pd.merge(FB_interet_id, lower_mau, on = 'id').drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find invalid cases where the upper and lower bound for all US are both 1000\n",
    "not_valid = pd.merge(upper_mau[['brand', 'all_US']], lower_mau[['brand', 'all_US']], on = 'brand')\n",
    "not_valid = not_valid.set_index('brand')\n",
    "not_valid = not_valid == 1000\n",
    "not_valid = list(not_valid[not_valid.sum(axis = 1) == 2].index.values)\n",
    "\n",
    "# Delete invalid cases\n",
    "upper_mau = upper_mau[~upper_mau['brand'].isin(not_valid)]\n",
    "lower_mau = lower_mau[~lower_mau['brand'].isin(not_valid)]\n",
    "\n",
    "# Save data for analysis\n",
    "upper_mau.to_csv('data/upper_mau.csv', index = False)\n",
    "lower_mau.to_csv('data/lower_mau.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
